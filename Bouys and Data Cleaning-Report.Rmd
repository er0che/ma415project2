---
title: 'Bouys and Data Cleaning: Report'
author: "Melody Shaff, Brian Clare, Elise Roche, and Carly Rose Willing"
date: "November 5, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setting Up Necessary Libraries

```{r}
# Setting up some libraries we'll need.
# The xlsx package requires Java and the rjava package. 

library(tidyverse)
library(stringr)
library(lubridate)
library(xlsx)
```

# Cape May Cleaning

As we repeated this section of code to clean each location's relevant data set, the comments describing the code can be found here. These comments describe all of the data cleaning, not exclusively Cape May, but we only included them here to avoid redundancy. A more extensive description of our processes and data cleaning efforts can be found later in this report in a section entitled: Report on Data Cleaning.  

```{r}
# First we set up the urls of the buoy data tables from the NDBC website:

str1 <- "http://www.ndbc.noaa.gov/view_text_file.php?filename=44009h"
str2 <- ".txt.gz&dir=data/historical/stdmet/"

years <- c(1984:2016)

urls <- str_c(str1, years, str2, sep = "")

# Now that we have urls, let's say what the filenames for each year will be:

filenames <- str_c("cm", years, sep = "")

# And how many we have:

N <- length(urls)

for (i in 1:N){
  
  # For each year of our data, get the table from that url and call it "file"
  
  assign(filenames[i], read_table(urls[i], col_names = TRUE))
  
  file <- get(filenames[i])
  
  # Before 1998 the year column is only 2 digits, YY, so let's get those to all match with 4 digits:
  
  colnames(file)[1] <-"YYYY"
  
  # If the first year entry is 2 digits, put 19 in front of it
  
  if(nchar(file[1,1]) == 2 & file[1,1] > 50 ) {
    file[1] <- lapply(file[1], function(x){str_c(19, x, sep = "")})
  }
  
  # Since we turned it into a string by doing that, let's get it back to a number:
  
  file$YYYY <- as.numeric(file$YYYY)
  
  # And then make sure all the rest of our data reads as numbers:
  
  file$MM <- as.numeric(file$MM)
  
  file$DD <- as.numeric(file$DD)
  
  file$hh <- as.numeric(file$hh)
  
  file$ATMP <- as.numeric(file$ATMP)
  
  file$WTMP <- as.numeric(file$WTMP)
  
  # Early years didn't have a minutes column.
  # For those with minutes, we want to add that column in with an entry of 0.
  # Otherwise, we only want 0 minutes if possible, but some years have entries at 50 minutes instead.
  # We check for this and add the column, when necessary, below. 
  
  if(is.element("mm", colnames(file))) {
    file$mm <- as.numeric(file$mm)
    file <- file %>% filter(mm == 00 | mm == 50)
  }
  
  else {
    file$mm <- 00
  }
  
  # Pulling out just the columns we care about, then making the date column:
  
  file <- file %>% select(YYYY, MM, DD, hh, mm, ATMP, WTMP)
  
  file$date <- make_datetime(year = file$YYYY, month = file$MM, day = file$DD, 
                             hour = file$hh, min = file$mm)
  
  # Putting our temporary 'file' into the main dataframe for this location, in this case CM, 
  # but each location has its own abbreviated dataframe title. 
  
  if(i == 1){
    CM <- file
  }
  
  else{
    CM <- rbind.data.frame(CM, file)
  }
  
}

# Now we have a few other actions to perform.
# We filter our data so we have only one entry per day, the closest entry to noon.

CM <- filter(CM, hh == 12 & mm == 00 | hh == 11 & mm == 50)

# Then we make the time difference column:

CM$timediff <- make_datetime(hour = CM$hh, min = CM$mm) - make_datetime(hour = 12, 
                                                                        min = 00) 
# Then we'll filter down to the columns we really want, recode the NAs, 
# and rename the columns  

CM <- select(CM, date, timediff, ATMP, WTMP)

CM$ATMP <- apply(CM[,3], MARGIN = 2, function(x){ifelse(x == 999.0, NA, x)})
CM$ATMP <- apply(CM[,3], MARGIN = 2, function(x){ifelse(x == 99.0, NA, x)})
CM$WTMP <- apply(CM[,4], MARGIN = 2, function(x){ifelse(x == 999.0, NA, x)})
CM$WTMP <- apply(CM[,4], MARGIN = 2, function(x){ifelse(x == 99.0, NA, x)})

colnames(CM)[3] <- "Air Temp"
colnames(CM)[4] <- "Sea Temp"
colnames(CM)[1] <- "Date"
colnames(CM)[2] <- "Time from Noon"

# There are a few more columns to add in for our group number, the type of data (ours is bouy data),
# the latitude, and the longitude of Cape May.

CM$Group <- 1
CM$Type <- "Buoy"
CM$Lat <- 38.5
CM$Long <- 74.7

# And then the columns need to be reordered:

CM <- CM[c("Group", "Type", "Date", "Time from Noon", "Lat", "Long", "Sea Temp", "Air Temp")]
```

# Molasses Reef 

Here we follow the same procedure to clean the data for the Molasses Reef location. As mentioned before, we did not add inline comments for this portion, but the comments above describe the code that is simply replicated here with only minor changes for name, etc. 

```{r}
str1 <- "http://www.ndbc.noaa.gov/view_text_file.php?filename=mlrf1h"
str2 <- ".txt.gz&dir=data/historical/stdmet/"

years <- c(1987:2016)

urls <- str_c(str1, years, str2, sep = "")

filenames <- str_c("mr", years, sep = "")

N <- length(urls)

for (i in 1:N){
  assign(filenames[i], read_table(urls[i], col_names = TRUE))
  
  file <- get(filenames[i])
  
  colnames(file)[1] <-"YYYY"
  
  if(nchar(file[1,1]) == 2 & file[1,1] > 50 ) {
    file[1] <- lapply(file[1], function(x){str_c(19, x, sep = "")})
  }
  
  file$YYYY <- as.numeric(file$YYYY)
  
  file$MM <- as.numeric(file$MM)
  
  file$DD <- as.numeric(file$DD)
  
  file$hh <- as.numeric(file$hh)
  
  file$ATMP <- as.numeric(file$ATMP)
  
  file$WTMP <- as.numeric(file$WTMP)
  
  if(is.element("mm", colnames(file))) {
    file$mm <- as.numeric(file$mm)
    file <- file %>% filter(mm == 00 | mm == 50)
  }
  
  else {
    file$mm <- 00
  }
  
  file <- file %>% select(YYYY, MM, DD, hh, mm, ATMP, WTMP)
  
  file$date <- make_datetime(year = file$YYYY, month = file$MM, day = file$DD, 
                             hour = file$hh, min = file$mm)
  
  if(i == 1){
    MR <- file
  }
  
  else{
    MR <- rbind.data.frame(MR, file)
  }
  
}

MR<-filter(MR, hh==12 & mm == 00 | hh == 11 & mm == 50)

MR$timediff <- make_datetime(hour = MR$hh, min = MR$mm) - make_datetime(hour = 12, min = 00) 

MR<-select(MR, date, timediff, ATMP, WTMP)

MR$ATMP <- apply(MR[,3], MARGIN = 2, function(x){ifelse(x == 999.0, NA, x)})
MR$ATMP <- apply(MR[,3], MARGIN = 2, function(x){ifelse(x == 99.0, NA, x)})
MR$WTMP <- apply(MR[,4], MARGIN = 2, function(x){ifelse(x == 999.0, NA, x)})
MR$WTMP <- apply(MR[,4], MARGIN = 2, function(x){ifelse(x == 99.0, NA, x)})

colnames(MR)[3] <- "Air Temp"
colnames(MR)[4] <- "Sea Temp"
colnames(MR)[1] <- "Date"
colnames(MR)[2] <- "Time from Noon"

MR$Group <- 1
MR$Type <- "Buoy"
MR$Lat <- 25.0
MR$Long <- 80.4

MR <- MR[c("Group", "Type", "Date", "Time from Noon", "Lat", "Long", "Sea Temp", "Air Temp")]
```

# Georges Bank

Below is our data cleaning procedure as applied to the Georges Bank buoy data. Again, comments describing this code can be found in the Cape May portion at the beginning of this report. 

```{r}
str1 <- "http://www.ndbc.noaa.gov/view_text_file.php?filename=44011h"
str2 <- ".txt.gz&dir=data/historical/stdmet/"

# This line looks slightly for Georges Bank because there is no bouy data for 2014 at this location. 
years <- c(1984:2013, 2015, 2016)

urls <- str_c(str1, years, str2, sep = "")

filenames <- str_c("gb", years, sep = "")

N <- length(urls)

for (i in 1:N){
  assign(filenames[i], read_table(urls[i], col_names = TRUE))
  
  file <- get(filenames[i])
  
  colnames(file)[1] <-"YYYY"
  
  if(nchar(file[1,1]) == 2 & file[1,1] > 50 ) {
    file[1] <- lapply(file[1], function(x){str_c(19, x, sep = "")})
  }
  
  file$YYYY <- as.numeric(file$YYYY)
  
  file$MM <- as.numeric(file$MM)
  
  file$DD <- as.numeric(file$DD)
  
  file$hh <- as.numeric(file$hh)
  
  file$ATMP <- as.numeric(file$ATMP)
  
  file$WTMP <- as.numeric(file$WTMP)
  
  if(is.element("mm", colnames(file))) {
    file$mm <- as.numeric(file$mm)
    file <- file %>% filter(mm == 00 | mm == 50)
  }
  
  else {
    file$mm <- 00
  }
  
  file <- file %>% select(YYYY, MM, DD, hh, mm, ATMP, WTMP)
  
  file$date <- make_datetime(year = file$YYYY, month = file$MM, day = file$DD, 
                             hour = file$hh, min = file$mm)
  
  if(i == 1){
    GB <- file
  }
  
  else{
    GB <- rbind.data.frame(GB, file)
  }
  
}

GB<-filter(GB, hh==12 & mm == 00 | hh == 11 & mm == 50)

GB$timediff <- make_datetime(hour = GB$hh, min = GB$mm) - make_datetime(hour = 12, min = 00) 

GB <- select(GB, date, timediff, ATMP, WTMP)

GB$ATMP <- apply(GB[,3], MARGIN = 2, function(x){ifelse(x == 999.0, NA, x)})
GB$ATMP <- apply(GB[,3], MARGIN = 2, function(x){ifelse(x == 99.0, NA, x)})
GB$WTMP <- apply(GB[,4], MARGIN = 2, function(x){ifelse(x == 999.0, NA, x)})
GB$WTMP <- apply(GB[,4], MARGIN = 2, function(x){ifelse(x == 99.0, NA, x)})

colnames(GB)[3] <- "Air Temp"
colnames(GB)[4] <- "Sea Temp"
colnames(GB)[1] <- "Date"
colnames(GB)[2] <- "Time from Noon"

GB$Group <- 1
GB$Type <- "Buoy"
GB$Lat <- 41.1
GB$Long <- 66.6

GB <- GB[c("Group", "Type", "Date", "Time from Noon", "Lat", "Long", "Sea Temp", "Air Temp")]
```

# Mid Gulf

Below is our data cleaning procedure as applied to the Cape Lookout buoy data. Again, comments describing this code can be found in the Cape May portion at the beginning of this report.

```{r}
str1 <- "http://www.ndbc.noaa.gov/view_text_file.php?filename=42001h"
str2 <- ".txt.gz&dir=data/historical/stdmet/"

years <- c(1984:2016)

urls <- str_c(str1, years, str2, sep = "")

filenames <- str_c("mg", years, sep = "")

N <- length(urls)

for (i in 1:N){
  assign(filenames[i], read_table(urls[i], col_names = TRUE))
  
  file <- get(filenames[i])
  
  colnames(file)[1] <-"YYYY"
  
  if(nchar(file[1,1]) == 2 & file[1,1] > 50 ) {
    file[1] <- lapply(file[1], function(x){str_c(19, x, sep = "")})
  }
  
  file$YYYY <- as.numeric(file$YYYY)
  
  file$MM <- as.numeric(file$MM)
  
  file$DD <- as.numeric(file$DD)
  
  file$hh <- as.numeric(file$hh)
  
  file$ATMP <- as.numeric(file$ATMP)
  
  file$WTMP <- as.numeric(file$WTMP)
  
  if(is.element("mm", colnames(file))) {
    file$mm <- as.numeric(file$mm)
    file <- file %>% filter(mm == 00 | mm == 50)
  }
  
  else {
    file$mm <- 00
  }
  
  file <- file %>% select(YYYY, MM, DD, hh, mm, ATMP, WTMP)
  
  file$date <- make_datetime(year = file$YYYY, month = file$MM, day = file$DD, 
                             hour = file$hh, min = file$mm)
  
  if(i == 1){
    MG <- file
  }
  
  else{
    MG <- rbind.data.frame(MG, file)
  }
  
}

MG <- filter(MG, hh == 12 & mm == 00 | hh == 11 & mm == 50)

MG$timediff <- make_datetime(hour = MG$hh, min = MG$mm) - make_datetime(hour = 12, min = 00) 

MG <- select(MG, date, timediff, ATMP, WTMP)

MG$ATMP <- apply(MG[,3], MARGIN = 2, function(x){ifelse(x == 999.0, NA, x)})
MG$ATMP <- apply(MG[,3], MARGIN = 2, function(x){ifelse(x == 99.0, NA, x)})
MG$WTMP <- apply(MG[,4], MARGIN = 2, function(x){ifelse(x == 999.0, NA, x)})
MG$WTMP <- apply(MG[,4], MARGIN = 2, function(x){ifelse(x == 99.0, NA, x)})

colnames(MG)[3] <- "Air Temp"
colnames(MG)[4] <- "Sea Temp"
colnames(MG)[1] <- "Date"
colnames(MG)[2] <- "Time from Noon"

MG$Group <- 1
MG$Type <- "Buoy"
MG$Lat <- 25.9
MG$Long <- 89.7

MG <- MG[c("Group", "Type", "Date", "Time from Noon", "Lat", "Long", "Sea Temp", "Air Temp")]
```

# Compilation Aid

Finally, we need to put these data frames into an xlsx file so that they can be compiled by our classmates. 

```{r}
write.xlsx(CM, "group1data.xlsx", sheetName = "Cape May - Buoy # 44009")
write.xlsx(MR, "group1data.xlsx", sheetName = "Molasses Reef - Buoy # MLRF1", append = TRUE)
write.xlsx(GB, "group1data.xlsx", sheetName = "Georges Bank - Buoy # 44011", append = TRUE)
write.xlsx(MG, "group1data.xlsx", sheetName = "Mid Gulf - Buoy # 42001", append = TRUE)
```

# Report on Data Cleaning

  Our raw buoy data was not ready for exploratory data analysis. The data had 14 columns and thousands of observations, many of which were superfluous for what we were focusing on for this project. Thus, our first step was to clean the data.
  In order to clean the data, we first needed to read the data into R. As we did not want to individually read in over 30 urls and we recognized the urls for each location differed by year only, we were able to create strings of the first half and second half of the url which remained consistent within a location. We used this to read in each location's data in a loop. Inside the loop we began the cleaning process. 
  First, we made sure all of the years were presented by 4 digit integers (as data before 1998 was represented by the last two digits, ex. 87 rather than 1987). To do this we needed to make the year a string and then change it back into a number. We then made sure all the rest of our data was in its numeric form, rather than represented as strings. Because early years were missing a minutes column, we added in the minutes for those as 00.  
  Second, we removed columns of no interest to us. For the purposes of this project we needed only: year (YYYY), month (MM), day (DD), hour (hh), minute (mm), air temperature (ATMP), and water temperature (WTMP). So, we selected only these columns. 
  Third, we needed a date column. Instead of the messy set of five columns representing time and date we transformed these (using make_datetime()) into a date object for each observation. 
  Fourth, we changed our temporary 'file' into the main data frame for this location. We used the temporary file to allow the code to be replicated, largely unchanged, between locations. However, at this point we are ready to establish the main data frame, unique to each location, and finish up our cleaning. Thus, we renamed file with the abbreviated notation for the location and bound that location's data into the new data frame.
  Fifth, we filtered out observations so we have only one for each day. Whenever possible we chose entries at noon, minute 0, but some instances only had minutes recorded at 50 and in those cases we selected 11:50 AM instead as this is closer to noon than 12:50 PM. We filtered for one entry, at (or close to) noon, per day.
  Sixth, as we now have a date object for each observation, we could make the new and necessary column for time difference (referring to the time, in number of seconds, from our data to noon). We did this simply by subtracting noon from the time at which the buoy data for each particular observation was taken.
  Seventh, we needed to filter for the columns we care about once more. Now that we've simplified the presentation of the date we can select only this one column, instead of the initial five. We also need to include our new time difference column. Thus, our final selected columns are: date (date), time difference from noon (timediff), air temperature (ATMP), and water temperature (WTMP). 
  Eighth, we want to re-code our NAs to register as such. Thus, we used apply() to address entries of 999.0 and 99.0 to appear instead as NA. 
  Ninth, we changed our column names to make them more readable and added in necessary new columns. We changed ATMP to Air Temp, WTMP to Sea Temp, date to Date, and timediff to Time from Noon. Next, we added in a column for our group number (1), the type of data we used (buoy data), the latitude, and the longitude of the location. 
  Finally, we reordered the columns. As the group compiling all of the groups' data sets needs uniformity we made sure to re-order our columns last to ensure that were easy to use for the next group. Thus, the columns are now: 1) Group, 2) Type, 3) Date, 4) Time from Noon, 5) Lat, 6) Long, 7) Sea Temp, 8) Air Temp.
  We used this same procedure for each of our locations' data. Our very last step was to put the cleaned data frames into an xlsx file. We did this by writing the file with the first location's data and adding the other three locations one by one as new sheets. 
  